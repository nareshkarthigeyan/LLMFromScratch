{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f033e306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2ce07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minbpe import BasicTokenizer\n",
    "\n",
    "tokenizer = BasicTokenizer()\n",
    "tokenizer.load(model_file='../output/tokenizer/my_tokenizer.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd44650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_size(tokenizer: BasicTokenizer) -> int:\n",
    "    vocab = tokenizer.vocab\n",
    "    special_tokens = tokenizer.special_tokens\n",
    "\n",
    "    return len(vocab) + len(special_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7e63e8",
   "metadata": {},
   "source": [
    "# Creating the Model - based on Andrej Karpathy's implementation of GPT-2 Model\n",
    "lesgooo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699bc1f1",
   "metadata": {},
   "source": [
    "## Step 1: Word & position embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d76719e",
   "metadata": {},
   "source": [
    "Let us convert the text into list of tokens. Each token has an ID from teh vocabulary. The shake of tensor is 1 x 6 because we have one scentence with 6 tokens. Next, we use the tokens to find corresponding embedding vector for each token. The vocab size is 1024, so each token uses it's ID to look up the right vector in the token embedding table. We do the same for positional embedding, which have 256 rows because block size is 256. This means the model cn only handly sequences with upto 256 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b538a0bc",
   "metadata": {},
   "source": [
    "After getting the token and postional embeddings, we add them together, this results in a tensor of sixe 1 * 6 * 768, where 1 is number of inputs, 6 is number of tokens, and 768 is the size of embedding vectors. Output of this is then sent to the block layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f817ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp312-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading torch-2.7.1-cp312-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Installing collected packages: sympy, torch\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.2\n",
      "    Uninstalling sympy-1.13.2:\n",
      "      Successfully uninstalled sympy-1.13.2\n",
      "Successfully installed sympy-1.14.0 torch-2.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d9e2f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(6969)\n",
    "\n",
    "block_size = 256\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "vocab_size = get_vocab_size(tokenizer)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217afae2",
   "metadata": {},
   "source": [
    "## 1. Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9816413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(\n",
    "            torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        _, T, _ = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x)  # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        weights = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
    "        weights = weights.masked_fill(\n",
    "            self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        weights = F.softmax(weights, dim=-1)  # (B, T, T)\n",
    "        weights = self.dropout(weights)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B,T,hs)\n",
    "        out = weights @ v  # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ca6acf",
   "metadata": {},
   "source": [
    "## 2. Multi Head Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b620e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.projection(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb38018",
   "metadata": {},
   "source": [
    "## 3. Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11f38342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int) -> None:\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, n_head: int) -> None:\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.self_attention = MultiHeadAttention(n_head, head_size)\n",
    "        self.feed_forward = FeedForward(n_embd)\n",
    "        self.layer_norm_1 = nn.LayerNorm(n_embd)\n",
    "        self.layer_norm_2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.self_attention(self.layer_norm_1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e51c1",
   "metadata": {},
   "source": [
    "## 4. Assembling the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d7583a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.final_layer_norm = nn.LayerNorm(n_embd)\n",
    "        self.final_linear_layer = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, input_tokens: torch.Tensor, targets: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            input_tokens: Tensor of token indices of shape (batch_size, sequence_length)\n",
    "            targets: Optional tensor of target token indices of same shape as input_tokens\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (logits, loss) where logits has shape (batch_size, sequence_length, vocab_size)\n",
    "            and loss is optional cross-entropy loss if targets are provided\n",
    "        \"\"\"\n",
    "\n",
    "        B, T = input_tokens.shape\n",
    "\n",
    "        # input_tokens and targets are both (B,T) tensor of integers\n",
    "        token_embedding = self.token_embedding_table(input_tokens)  # (B,T,C)\n",
    "        positional_embedding = self.position_embedding_table(\n",
    "            torch.arange(T, device=device))  # (T,C)\n",
    "        x = token_embedding + positional_embedding  # (B,T,C)\n",
    "        x = self.blocks(x)  # (B,T,C)\n",
    "        x = self.final_layer_norm(x)  # (B,T,C)\n",
    "        logits = self.final_linear_layer(x)  # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, input_tokens: torch.Tensor, max_new_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "                Generate new tokens given a context.\n",
    "\n",
    "                Args:\n",
    "                        input_tokens: Starting token indices of shape (batch_size, sequence_length)\n",
    "                        max_new_tokens: Number of new tokens to generate\n",
    "\n",
    "                Returns:\n",
    "                        Tensor of token indices of shape (batch_size, sequence_length + max_new_tokens)\n",
    "                \"\"\"\n",
    "\n",
    "        # input_tokens is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop input_tokens to the last block_size tokens\n",
    "            cropped_input = input_tokens[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, _ = self(cropped_input)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            input_tokens = torch.cat(\n",
    "                (input_tokens, idx_next), dim=1)  # (B, T+1)\n",
    "        return input_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821ab3f",
   "metadata": {},
   "source": [
    "# 5. Parameters and Dummy Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07abd10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.5264 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPTLanguageModel()\n",
    "model = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7877d732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 1024]) None\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "seq_length = 6\n",
    "x = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
    "x = x.to(device)\n",
    "\n",
    "logits, loss = model(x)\n",
    "print(logits.shape, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3289027b",
   "metadata": {},
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a6e4c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├─ token_embedding_table: Embedding (393,216 parameters)\n",
      "├─ position_embedding_table: Embedding (98,304 parameters)\n",
      "├─ blocks: Sequential (10,639,872 parameters)\n",
      "│  ├─ 0: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "│  ├─ 1: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "│  ├─ 2: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "│  ├─ 3: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "│  ├─ 4: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "│  ├─ 5: Block (1,773,312 parameters)\n",
      "│  │  ├─ self_attention: MultiHeadAttention (590,208 parameters)\n",
      "│  │  │  ├─ heads: ModuleList (442,368 parameters)\n",
      "│  │  │  │  ├─ 0: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 1: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 2: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 3: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 4: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  │  ├─ 5: Head (73,728 parameters)\n",
      "│  │  │  │  │  ├─ key: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ query: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ value: Linear (24,576 parameters)\n",
      "│  │  │  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  │  ├─ projection: Linear (147,840 parameters)\n",
      "│  │  │  ├─ dropout: Dropout (0 parameters)\n",
      "│  │  ├─ feed_forward: FeedForward (1,181,568 parameters)\n",
      "│  │  │  ├─ net: Sequential (1,181,568 parameters)\n",
      "│  │  │  │  ├─ 0: Linear (591,360 parameters)\n",
      "│  │  │  │  ├─ 1: ReLU (0 parameters)\n",
      "│  │  │  │  ├─ 2: Linear (590,208 parameters)\n",
      "│  │  │  │  ├─ 3: Dropout (0 parameters)\n",
      "│  │  ├─ layer_norm_1: LayerNorm (768 parameters)\n",
      "│  │  ├─ layer_norm_2: LayerNorm (768 parameters)\n",
      "├─ final_layer_norm: LayerNorm (768 parameters)\n",
      "├─ final_linear_layer: Linear (394,240 parameters)\n"
     ]
    }
   ],
   "source": [
    "def print_model_structure(model: torch.nn.Module, indent: str = '') -> None:\n",
    "    \"\"\"\n",
    "    Custom function to print model structure in a hierarchical format\n",
    "    \"\"\"\n",
    "    for name, child in model.named_children():\n",
    "        params = sum(p.numel() for p in child.parameters())\n",
    "        print(f\"{indent}├─ {name}: {child.__class__.__name__} ({params:,} parameters)\")\n",
    "        print_model_structure(child, indent + '│  ')\n",
    "\n",
    "\n",
    "print_model_structure(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83cc4b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>Trainable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>token_embedding_table</td>\n",
       "      <td>Embedding</td>\n",
       "      <td>393216</td>\n",
       "      <td>393216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>position_embedding_table</td>\n",
       "      <td>Embedding</td>\n",
       "      <td>98304</td>\n",
       "      <td>98304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blocks.0.self_attention.heads.0.key</td>\n",
       "      <td>Linear</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blocks.0.self_attention.heads.0.query</td>\n",
       "      <td>Linear</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blocks.0.self_attention.heads.0.value</td>\n",
       "      <td>Linear</td>\n",
       "      <td>24576</td>\n",
       "      <td>24576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>blocks.5.feed_forward.net.3</td>\n",
       "      <td>Dropout</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>blocks.5.layer_norm_1</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>blocks.5.layer_norm_2</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>final_layer_norm</td>\n",
       "      <td>LayerNorm</td>\n",
       "      <td>768</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>final_linear_layer</td>\n",
       "      <td>Linear</td>\n",
       "      <td>394240</td>\n",
       "      <td>394240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Layer Name       Type  Parameters  Trainable\n",
       "0                    token_embedding_table  Embedding      393216     393216\n",
       "1                 position_embedding_table  Embedding       98304      98304\n",
       "2      blocks.0.self_attention.heads.0.key     Linear       24576      24576\n",
       "3    blocks.0.self_attention.heads.0.query     Linear       24576      24576\n",
       "4    blocks.0.self_attention.heads.0.value     Linear       24576      24576\n",
       "..                                     ...        ...         ...        ...\n",
       "191            blocks.5.feed_forward.net.3    Dropout           0          0\n",
       "192                  blocks.5.layer_norm_1  LayerNorm         768        768\n",
       "193                  blocks.5.layer_norm_2  LayerNorm         768        768\n",
       "194                       final_layer_norm  LayerNorm         768        768\n",
       "195                     final_linear_layer     Linear      394240     394240\n",
       "\n",
       "[196 rows x 4 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_model_stats(model: torch.nn.Module) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a DataFrame with detailed layer statistics\n",
    "    \"\"\"\n",
    "    stats = []\n",
    "    for name, module in model.named_modules():\n",
    "        if len(list(module.children())) == 0:  # Only leaf modules\n",
    "            params = sum(p.numel() for p in module.parameters())\n",
    "            stats.append({\n",
    "                'Layer Name': name,\n",
    "                'Type': module.__class__.__name__,\n",
    "                'Parameters': params,\n",
    "                'Trainable': sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "            })\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "\n",
    "stats_df = get_model_stats(model)\n",
    "stats_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
